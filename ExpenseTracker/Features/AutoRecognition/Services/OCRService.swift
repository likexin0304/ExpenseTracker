import UIKit
import Vision
import Combine

/**
 * OCRæ–‡å­—è¯†åˆ«æœåŠ¡ - Phase 3 ä¼˜åŒ–ç‰ˆæœ¬
 * è´Ÿè´£ä»å›¾åƒä¸­è¯†åˆ«æ–‡å­—å†…å®¹
 */
class OCRService: ObservableObject {
    
    // MARK: - Published Properties
    
    /// æ˜¯å¦æ­£åœ¨è¯†åˆ«
    @Published var isRecognizing: Bool = false
    
    /// è¯†åˆ«è¿›åº¦ (0.0 - 1.0)
    @Published var recognitionProgress: Double = 0.0
    
    // MARK: - Private Properties
    
    /// è¯†åˆ«é˜Ÿåˆ—
    private let recognitionQueue = DispatchQueue(label: "com.expensetracker.ocr", qos: .userInitiated)
    
    /// Combineè®¢é˜…
    private var cancellables = Set<AnyCancellable>()
    
    /// OCRé…ç½®
    private let configuration = OCRConfiguration()
    
    /// æ€§èƒ½ç›‘æ§
    private var performanceMonitor = OCRPerformanceMonitor()
    
    // MARK: - Singleton
    
    static let shared = OCRService()
    
    private init() {}
    
    // MARK: - Public Methods
    
    /**
     * è¯†åˆ«å›¾åƒä¸­çš„æ–‡å­— - Phase 3 ä¼˜åŒ–ç‰ˆæœ¬
     * - Parameter image: è¦è¯†åˆ«çš„å›¾åƒ
     * - Returns: OCRè¯†åˆ«ç»“æœ
     */
    func recognizeText(from image: UIImage) async -> Result<OCRData, AutoRecognitionError> {
        print("ğŸ” å¼€å§‹OCRæ–‡å­—è¯†åˆ«")
        let startTime = Date()
        
        await MainActor.run {
            isRecognizing = true
            recognitionProgress = 0.0
        }
        
        defer {
            Task { @MainActor in
                isRecognizing = false
                recognitionProgress = 0.0
            }
        }
        
        return await withCheckedContinuation { continuation in
            recognitionQueue.async {
                self.performOCRRecognition(image: image) { result in
                    let processingTime = Date().timeIntervalSince(startTime)
                    self.performanceMonitor.recordRecognition(
                        success: result.isSuccess,
                        processingTime: processingTime
                    )
                    continuation.resume(returning: result)
                }
            }
        }
    }
    
    /**
     * æ‰¹é‡è¯†åˆ«å¤šå¼ å›¾ç‰‡
     * - Parameter images: è¦è¯†åˆ«çš„å›¾ç‰‡æ•°ç»„
     * - Returns: OCRè¯†åˆ«ç»“æœæ•°ç»„
     */
    func recognizeText(in images: [UIImage]) async -> [Result<OCRData, AutoRecognitionError>] {
        print("ğŸ” å¼€å§‹æ‰¹é‡OCRè¯†åˆ«ï¼Œå…±\(images.count)å¼ å›¾ç‰‡")
        
        var results: [Result<OCRData, AutoRecognitionError>] = []
        
        for (index, image) in images.enumerated() {
            await MainActor.run {
                recognitionProgress = Double(index) / Double(images.count)
            }
            
            let result = await recognizeText(from: image)
            results.append(result)
        }
        
        await MainActor.run {
            recognitionProgress = 1.0
        }
        
        return results
    }
    
    // MARK: - Private Methods
    
    /**
     * æ‰§è¡ŒOCRè¯†åˆ« - Phase 3 ä¼˜åŒ–ç‰ˆæœ¬
     */
    private func performOCRRecognition(
        image: UIImage,
        completion: @escaping (Result<OCRData, AutoRecognitionError>) -> Void
    ) {
        // æ›´æ–°è¿›åº¦
        Task { @MainActor in
            recognitionProgress = 0.1
        }
        
        // 1. å›¾åƒé¢„å¤„ç†
        guard let preprocessedImage = preprocessImage(image) else {
            completion(.failure(.ocrFailed("å›¾åƒé¢„å¤„ç†å¤±è´¥")))
            return
        }
        
        Task { @MainActor in
            recognitionProgress = 0.2
        }
        
        // 2. åˆ›å»ºVisionè¯·æ±‚
        let request = createVisionRequest { [weak self] result in
            switch result {
            case .success(let ocrData):
                completion(.success(ocrData))
            case .failure(let error):
                completion(.failure(error))
            }
        }
        
        Task { @MainActor in
            recognitionProgress = 0.3
        }
        
        // 3. æ‰§è¡Œè¯†åˆ«
        guard let cgImage = preprocessedImage.cgImage else {
            completion(.failure(.ocrFailed("æ— æ³•è·å–å›¾åƒæ•°æ®")))
            return
        }
        
        let handler = VNImageRequestHandler(cgImage: cgImage, options: [:])
        
        do {
            try handler.perform([request])
        } catch {
            print("âŒ OCRè¯†åˆ«å¤±è´¥: \(error.localizedDescription)")
            completion(.failure(.ocrFailed("OCRè¯†åˆ«å¤±è´¥: \(error.localizedDescription)")))
        }
    }
    
    /**
     * å›¾åƒé¢„å¤„ç† - Phase 3 æ–°å¢
     */
    private func preprocessImage(_ image: UIImage) -> UIImage? {
        guard let cgImage = image.cgImage else { return nil }
        
        // è°ƒæ•´å›¾åƒå¤§å°ä»¥ä¼˜åŒ–è¯†åˆ«æ€§èƒ½
        let targetSize = calculateOptimalSize(for: image.size)
        
        UIGraphicsBeginImageContextWithOptions(targetSize, false, 1.0)
        defer { UIGraphicsEndImageContext() }
        
        // ç»˜åˆ¶è°ƒæ•´åçš„å›¾åƒ
        image.draw(in: CGRect(origin: .zero, size: targetSize))
        
        guard let resizedImage = UIGraphicsGetImageFromCurrentImageContext() else {
            return image
        }
        
        // åº”ç”¨å›¾åƒå¢å¼ºæ»¤é•œ
        return applyImageEnhancement(to: resizedImage) ?? resizedImage
    }
    
    /**
     * è®¡ç®—æœ€ä¼˜å›¾åƒå°ºå¯¸
     */
    private func calculateOptimalSize(for originalSize: CGSize) -> CGSize {
        let maxDimension: CGFloat = 2048
        let minDimension: CGFloat = 512
        
        let maxOriginal = max(originalSize.width, originalSize.height)
        let minOriginal = min(originalSize.width, originalSize.height)
        
        // å¦‚æœå›¾åƒå¤ªå¤§ï¼ŒæŒ‰æ¯”ä¾‹ç¼©å°
        if maxOriginal > maxDimension {
            let scale = maxDimension / maxOriginal
            return CGSize(
                width: originalSize.width * scale,
                height: originalSize.height * scale
            )
        }
        
        // å¦‚æœå›¾åƒå¤ªå°ï¼ŒæŒ‰æ¯”ä¾‹æ”¾å¤§
        if minOriginal < minDimension {
            let scale = minDimension / minOriginal
            return CGSize(
                width: originalSize.width * scale,
                height: originalSize.height * scale
            )
        }
        
        return originalSize
    }
    
    /**
     * åº”ç”¨å›¾åƒå¢å¼ºæ»¤é•œ
     */
    private func applyImageEnhancement(to image: UIImage) -> UIImage? {
        guard let cgImage = image.cgImage else { return nil }
        
        let context = CIContext()
        let ciImage = CIImage(cgImage: cgImage)
        
        // åº”ç”¨å¯¹æ¯”åº¦å’Œäº®åº¦è°ƒæ•´
        guard let contrastFilter = CIFilter(name: "CIColorControls") else { return image }
        contrastFilter.setValue(ciImage, forKey: kCIInputImageKey)
        contrastFilter.setValue(1.2, forKey: kCIInputContrastKey) // å¢åŠ å¯¹æ¯”åº¦
        contrastFilter.setValue(0.1, forKey: kCIInputBrightnessKey) // è½»å¾®å¢åŠ äº®åº¦
        
        guard let contrastOutput = contrastFilter.outputImage else { return image }
        
        // åº”ç”¨é”åŒ–æ»¤é•œ
        guard let sharpenFilter = CIFilter(name: "CIUnsharpMask") else {
            // å¦‚æœé”åŒ–å¤±è´¥ï¼Œè¿”å›å¯¹æ¯”åº¦è°ƒæ•´åçš„å›¾åƒ
            guard let outputCGImage = context.createCGImage(contrastOutput, from: contrastOutput.extent) else {
                return image
            }
            return UIImage(cgImage: outputCGImage)
        }
        
        sharpenFilter.setValue(contrastOutput, forKey: kCIInputImageKey)
        sharpenFilter.setValue(0.5, forKey: kCIInputIntensityKey)
        sharpenFilter.setValue(2.5, forKey: kCIInputRadiusKey)
        
        guard let finalOutput = sharpenFilter.outputImage,
              let outputCGImage = context.createCGImage(finalOutput, from: finalOutput.extent) else {
            return image
        }
        
        return UIImage(cgImage: outputCGImage)
    }
    
    /**
     * åˆ›å»ºVisionè¯†åˆ«è¯·æ±‚ - Phase 3 ä¼˜åŒ–ç‰ˆæœ¬
     */
    private func createVisionRequest(
        completion: @escaping (Result<OCRData, AutoRecognitionError>) -> Void
    ) -> VNRecognizeTextRequest {
        
        let request = VNRecognizeTextRequest { [weak self] request, error in
            Task { @MainActor in
                self?.recognitionProgress = 0.8
            }
            
            if let error = error {
                print("âŒ Visionè¯†åˆ«é”™è¯¯: \(error.localizedDescription)")
                completion(.failure(.ocrFailed("Visionè¯†åˆ«é”™è¯¯: \(error.localizedDescription)")))
                return
            }
            
            guard let observations = request.results as? [VNRecognizedTextObservation] else {
                completion(.failure(.ocrFailed("æ— æ³•è·å–è¯†åˆ«ç»“æœ")))
                return
            }
            
            Task { @MainActor in
                self?.recognitionProgress = 0.9
            }
            
            // å¤„ç†è¯†åˆ«ç»“æœ
            self?.processRecognitionResults(observations, completion: completion)
        }
        
        // é…ç½®è¯†åˆ«å‚æ•° - Phase 3 ä¼˜åŒ–
        request.recognitionLevel = .accurate // ä½¿ç”¨é«˜ç²¾åº¦è¯†åˆ«
        request.usesLanguageCorrection = true // å¯ç”¨è¯­è¨€æ ¡æ­£
        
        // è®¾ç½®æ”¯æŒçš„è¯­è¨€
        if #available(iOS 16.0, *) {
            request.recognitionLanguages = ["zh-Hans", "zh-Hant", "en-US"] // ç®€ä½“ä¸­æ–‡ã€ç¹ä½“ä¸­æ–‡ã€è‹±æ–‡
        } else {
            // iOS 15åŠä»¥ä¸‹ç‰ˆæœ¬çš„å…¼å®¹å¤„ç†
            request.recognitionLanguages = ["zh", "en"]
        }
        
        // è®¾ç½®è‡ªå®šä¹‰è¯æ±‡ï¼ˆå¸¸è§çš„é‡‘é¢å’Œå•†å®¶å…³é”®è¯ï¼‰
        if #available(iOS 15.0, *) {
            request.customWords = configuration.customWords
        }
        
        return request
    }
    
    /**
     * å¤„ç†è¯†åˆ«ç»“æœ - Phase 3 ä¼˜åŒ–ç‰ˆæœ¬
     */
    private func processRecognitionResults(
        _ observations: [VNRecognizedTextObservation],
        completion: @escaping (Result<OCRData, AutoRecognitionError>) -> Void
    ) {
        var textBlocks: [OCRTextBlock] = []
        var overallConfidence: Float = 0.0
        var totalConfidence: Float = 0.0
        var validObservations = 0
        
        for observation in observations {
            // è·å–æœ€ä½³å€™é€‰æ–‡æœ¬
            guard let topCandidate = observation.topCandidates(1).first else { continue }
            
            let text = topCandidate.string
            let confidence = topCandidate.confidence
            
            // è¿‡æ»¤æ‰ç½®ä¿¡åº¦è¿‡ä½çš„æ–‡æœ¬
            guard confidence > configuration.minimumConfidence else { continue }
            
            // åˆ†ææ–‡æœ¬ç±»å‹
            let textType = analyzeTextType(text)
            
            // è·å–æ–‡æœ¬è¾¹ç•Œæ¡†
            let boundingBox = observation.boundingBox
            
            let textBlock = OCRTextBlock(
                text: text,
                confidence: confidence,
                boundingBox: boundingBox,
                textType: textType,
                isPotentialAmount: textType == .amount || textType == .currency,
                isPotentialMerchant: textType == .merchant || textType == .header
            )
            
            textBlocks.append(textBlock)
            totalConfidence += confidence
            validObservations += 1
        }
        
        // è®¡ç®—æ•´ä½“ç½®ä¿¡åº¦
        overallConfidence = validObservations > 0 ? totalConfidence / Float(validObservations) : 0.0
        
        Task { @MainActor in
            recognitionProgress = 1.0
        }
        
        // éªŒè¯è¯†åˆ«ç»“æœ
        guard !textBlocks.isEmpty else {
            completion(.failure(.ocrFailed("æœªè¯†åˆ«åˆ°ä»»ä½•æ–‡æœ¬")))
            return
        }
        
        guard overallConfidence > configuration.minimumOverallConfidence else {
            completion(.failure(.ocrFailed("è¯†åˆ«ç½®ä¿¡åº¦è¿‡ä½: \(String(format: "%.2f", overallConfidence))")))
            return
        }
        
        // åå¤„ç†ï¼šæ–‡æœ¬æ¸…ç†å’Œä¼˜åŒ–
        let cleanedTextBlocks = postProcessTextBlocks(textBlocks)
        
        let ocrData = OCRData(
            textBlocks: cleanedTextBlocks,
            confidence: overallConfidence,
            processingTime: Date().timeIntervalSince(Date()),
            imageSize: CGSize.zero // è¿™é‡Œå¯ä»¥ä¼ å…¥å®é™…å›¾åƒå°ºå¯¸
        )
        
        print("âœ… OCRè¯†åˆ«å®Œæˆ")
        print("ğŸ“ è¯†åˆ«åˆ° \(cleanedTextBlocks.count) ä¸ªæ–‡æœ¬å—")
        print("ğŸ¯ æ•´ä½“ç½®ä¿¡åº¦: \(String(format: "%.2f", overallConfidence))")
        print("ğŸ“„ è¯†åˆ«æ–‡æœ¬: \(cleanedTextBlocks.map { $0.text }.joined(separator: " | "))")
        
        completion(.success(ocrData))
    }
    
    /**
     * åˆ†ææ–‡æœ¬ç±»å‹ - Phase 3 æ–°å¢
     */
    private func analyzeTextType(_ text: String) -> OCRTextType {
        let normalizedText = text.lowercased().trimmingCharacters(in: .whitespacesAndNewlines)
        
        // é‡‘é¢æ¨¡å¼
        let amountPatterns = [
            "^[Â¥$â‚¬]?[0-9,]+\\.?[0-9]*$",
            "^[0-9,]+\\.[0-9]{2}å…ƒ?$",
            "^[0-9,]+å…ƒ$"
        ]
        
        for pattern in amountPatterns {
            if normalizedText.range(of: pattern, options: .regularExpression) != nil {
                return .amount
            }
        }
        
        // è´§å¸ç¬¦å·
        if normalizedText.contains("Â¥") || normalizedText.contains("$") || normalizedText.contains("â‚¬") {
            return .currency
        }
        
        // æ—¥æœŸæ¨¡å¼
        let datePatterns = [
            "\\d{4}-\\d{2}-\\d{2}",
            "\\d{4}/\\d{2}/\\d{2}",
            "\\d{4}å¹´\\d{1,2}æœˆ\\d{1,2}æ—¥"
        ]
        
        for pattern in datePatterns {
            if normalizedText.range(of: pattern, options: .regularExpression) != nil {
                return .date
            }
        }
        
        // æ—¶é—´æ¨¡å¼
        if normalizedText.range(of: "\\d{1,2}:\\d{2}", options: .regularExpression) != nil {
            return .time
        }
        
        // å•†å®¶åç§°ï¼ˆé€šå¸¸è¾ƒé•¿ä¸”åŒ…å«ç‰¹å®šå…³é”®è¯ï¼‰
        let merchantKeywords = ["åº—", "é¤å…", "å…¬å¸", "æœ‰é™", "è¶…å¸‚", "å•†åœº", "ä¸­å¿ƒ"]
        if merchantKeywords.contains(where: { normalizedText.contains($0) }) && text.count > 3 {
            return .merchant
        }
        
        // æ ‡é¢˜ï¼ˆé€šå¸¸è¾ƒçŸ­ä¸”åœ¨é¡¶éƒ¨ï¼‰
        if text.count <= 10 && !normalizedText.contains(" ") {
            return .header
        }
        
        return .general
    }
    
    /**
     * åå¤„ç†æ–‡æœ¬å— - Phase 3 æ–°å¢
     */
    private func postProcessTextBlocks(_ textBlocks: [OCRTextBlock]) -> [OCRTextBlock] {
        return textBlocks.compactMap { block in
            let cleanedText = cleanText(block.text)
            
            // è¿‡æ»¤æ‰è¿‡çŸ­æˆ–æ— æ„ä¹‰çš„æ–‡æœ¬
            guard cleanedText.count >= 1 && !cleanedText.trimmingCharacters(in: .whitespacesAndNewlines).isEmpty else {
                return nil
            }
            
            return OCRTextBlock(
                text: cleanedText,
                confidence: block.confidence,
                boundingBox: block.boundingBox,
                textType: block.textType,
                isPotentialAmount: block.isPotentialAmount,
                isPotentialMerchant: block.isPotentialMerchant
            )
        }
    }
    
    /**
     * æ¸…ç†æ–‡æœ¬ - Phase 3 æ–°å¢
     */
    private func cleanText(_ text: String) -> String {
        var cleaned = text
        
        // ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
        cleaned = cleaned.trimmingCharacters(in: .whitespacesAndNewlines)
        cleaned = cleaned.replacingOccurrences(of: "\\s+", with: " ", options: .regularExpression)
        
        // ä¿®æ­£å¸¸è§çš„OCRé”™è¯¯
        let corrections = [
            "O": "0", // Oè¯¯è¯†åˆ«ä¸º0
            "l": "1", // lè¯¯è¯†åˆ«ä¸º1
            "S": "5", // Sè¯¯è¯†åˆ«ä¸º5
            "G": "6", // Gè¯¯è¯†åˆ«ä¸º6
            "B": "8", // Bè¯¯è¯†åˆ«ä¸º8
        ]
        
        // åº”ç”¨ä¿®æ­£
        for (wrong, correct) in corrections {
            cleaned = cleaned.replacingOccurrences(of: wrong, with: correct)
        }
        
        return cleaned
    }
}

// MARK: - Supporting Types

/**
 * OCRé…ç½® - Phase 3 æ–°å¢
 */
struct OCRConfiguration {
    /// æœ€å°æ–‡æœ¬ç½®ä¿¡åº¦
    let minimumConfidence: Float = 0.3
    
    /// æœ€å°æ•´ä½“ç½®ä¿¡åº¦
    let minimumOverallConfidence: Float = 0.5
    
    /// è‡ªå®šä¹‰è¯æ±‡
    let customWords: [String] = [
        // é‡‘é¢ç›¸å…³
        "å…ƒ", "Â¥", "$", "â‚¬", "æ€»è®¡", "åˆè®¡", "åº”ä»˜", "å®ä»˜", "å°è®¡",
        // å•†å®¶ç›¸å…³
        "é¤å…", "è¶…å¸‚", "å•†åœº", "è¯åº—", "åŠ æ²¹ç«™", "åœè½¦åœº",
        // æ”¯ä»˜ç›¸å…³
        "å¾®ä¿¡", "æ”¯ä»˜å®", "é“¶è¡Œå¡", "ç°é‡‘", "åˆ·å¡"
    ]
}



/**
 * OCRæ€§èƒ½ç›‘æ§ - Phase 3 æ–°å¢
 */
class OCRPerformanceMonitor {
    private var recognitionCount = 0
    private var successCount = 0
    private var totalProcessingTime: TimeInterval = 0
    
    func recordRecognition(success: Bool, processingTime: TimeInterval) {
        recognitionCount += 1
        totalProcessingTime += processingTime
        
        if success {
            successCount += 1
        }
        
        // æ¯10æ¬¡è¯†åˆ«è¾“å‡ºä¸€æ¬¡ç»Ÿè®¡
        if recognitionCount % 10 == 0 {
            let successRate = Double(successCount) / Double(recognitionCount) * 100
            let avgTime = totalProcessingTime / Double(recognitionCount) * 1000
            
            print("ğŸ“Š OCRæ€§èƒ½ç»Ÿè®¡:")
            print("   è¯†åˆ«æ¬¡æ•°: \(recognitionCount)")
            print("   æˆåŠŸç‡: \(String(format: "%.1f", successRate))%")
            print("   å¹³å‡è€—æ—¶: \(String(format: "%.1f", avgTime))ms")
        }
    }
}

// MARK: - Result Extension

extension Result {
    var isSuccess: Bool {
        switch self {
        case .success:
            return true
        case .failure:
            return false
        }
    }
} 